# AI Agent Playground - made with Resilient LLM

A simple playground to test and build with [`ResilientLLM`](https://github.com/gitcommitshow/resilient-llm)

![Demo Screenshot](./demo.jpg)

## Features

This project can act as the starting point to test your prompt, workflow, and ResilientLLM behavior. Recommended to try this before starting your AI agent project.

- TK

## Project Structure

```
server/                    --Backend files--
â”œâ”€â”€ app.js                 # Express server with ResilientLLM
â””â”€â”€ devutility.js          # Development utilities
client/                    --Frontend files--
â”œâ”€â”€ index.html             # Main HTML file
â”œâ”€â”€ styles.css             # Styling
â”œâ”€â”€ api.js                 # API integration with the express API backend
â”œâ”€â”€ components/             # UI components
â”‚   â”œâ”€â”€ ChatPanel.js
â”‚   â”œâ”€â”€ MessageInput.js
â”‚   â”œâ”€â”€ MessageRenderer.js
â”‚   â”œâ”€â”€ Notification.js
â”‚   â”œâ”€â”€ PromptHeader.js
â”‚   â”œâ”€â”€ PromptsSidebar.js
â”‚   â”œâ”€â”€ SettingsDrawer.js
â”‚   â”œâ”€â”€ StatusBar.js
â”‚   â”œâ”€â”€ SystemPromptPanel.js
â”‚   â””â”€â”€ VersionBar.js
â”œâ”€â”€ core/                  # Core application logic
â”‚   â”œâ”€â”€ App.js
â”‚   â”œâ”€â”€ State.js
â”‚   â””â”€â”€ Storage.js
â”œâ”€â”€ models/                # Data models
â”‚   â”œâ”€â”€ Conversation.js
â”‚   â”œâ”€â”€ Message.js
â”‚   â”œâ”€â”€ Prompt.js
â”‚   â””â”€â”€ Version.js
â””â”€â”€ utils/                 # Utility functions
    â”œâ”€â”€ datetime.js
    â”œâ”€â”€ dom.js
    â””â”€â”€ markdown.js
```

## Quick Start

### 1. Clone and Setup

```bash
git clone https://github.com/gitcommitshow/resilient-llm
cd resilient-llm/examples/playground-js
```

### 2. Install Dependencies

```bash
npm install
```

### 3. Set Environment Variables

Set your API key and choose the default LLM service and model:

```bash
# OpenAI
export OPENAI_API_KEY=your_key_here
export AI_SERVICE=openai
export AI_MODEL=gpt-4o-mini

# Or Anthropic
export ANTHROPIC_API_KEY=your_key_here
export AI_SERVICE=anthropic
export AI_MODEL=claude-3-5-sonnet-20240620

# Or Gemini
export GEMINI_API_KEY=your_key_here
export AI_SERVICE=gemini
export AI_MODEL=gemini-2.0-flash
```

### 4. Start the Server

```bash
npm run dev
```

The server will start on `http://localhost:3000` and automatically serve the client files.

### 5. Open in Browser

Navigate to **`http://localhost:3000`** in your browser.

<details>
<summary><strong>Want to preview in the VSCode/Cursor editor directly?</strong></summary>

- Install [Live Preview extension](https://marketplace.cursorapi.com/items/?itemName=ms-vscode.live-server)
- Right-click on `client/index.html` â†’ **"Show Preview"**

**Note:** The server must be running for the preview to work, as it serves the client files and handles API requests.

</details>

----

ğŸ Discovered a bug? [Create an issue](https://github.com/gitcommitshow/resilient-llm/issues/new)

## License 

MIT License