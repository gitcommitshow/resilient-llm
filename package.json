{
  "name": "resilient-llm",
  "version": "1.1.0",
  "description": "ResilientLLM is a resilient, unified LLM interface with in-built circuit breaker, token bucket rate limiting, caching, and adaptive retry with dynamic backoff support.",
  "main": "index.js",
  "type": "module",
  "engines": {
    "node": ">=20.0.0"
  },
  "scripts": {
    "test": "mocha 'test/**/*.test.js'",
    "test:watch": "mocha --watch 'test/**/*.test.js'",
    "test:coverage": "nyc mocha 'test/**/*.test.js'",
    "test:e2e": "mocha 'test/**/*.e2e.test.js' --timeout 60000",
    "example:chat": "rm -rf resilient-llm-*.tgz && npm pack --pack-destination . && mv resilient-llm-*.tgz resilient-llm-local.tgz && cd examples/chat-basic && rm -rf node_modules/resilient-llm && npm install ../../resilient-llm-local.tgz && RESILIENT_LLM_SOURCE=local npm run dev",
    "example:chat:npm": "cd examples/chat-basic && npm install resilient-llm@latest && npm run dev"
  },
  "keywords": [
    "llm",
    "ai",
    "rate-limit",
    "fail-safe",
    "fault-tolerant",
    "failover",
    "retry",
    "throttle",
    "circuit-breaker",
    "backoff"
  ],
  "author": "",
  "license": "MIT",
  "dependencies": {
    "js-tiktoken": "^1.0.21"
  },
  "devDependencies": {
    "chai": "^6.0.1",
    "chai-as-promised": "^8.0.2",
    "mocha": "^11.7.1",
    "nyc": "^17.1.0",
    "sinon": "^21.0.0"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/gitcommitshow/resilient-llm"
  }
}
